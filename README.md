<p align="center">
    <picture>
        <source media="(prefers-color-scheme: dark)" srcset="https://github.com/tassa-yoniso-manasi-karoto/langkit/raw/refs/heads/master/internal/drawing-blackBg.webp">
        <source media="(prefers-color-scheme: light)" srcset="https://github.com/tassa-yoniso-manasi-karoto/langkit/raw/refs/heads/master/internal/drawing-whiteBg.webp">
        <img width=375 src="https://github.com/tassa-yoniso-manasi-karoto/langkit/raw/refs/heads/master/internal/drawing-whiteBg.webp">
    </picture>
</p>


### Status: prerelease

Langkit is an all-in-one tool designed to **facilitate language learning from native media content** using a collection of diverse features to transform movies, TV shows, etc., into **easily â€˜digestibleâ€™ material**. It was made with scalability, fault-tolerance in mind and supports automatic subtitle detection, bulk/recursive directory processing, seamless resumption of previously interrupted processing runs and multiple native (reference) language fallback.

### Features

- **Subs2cards**: Make Anki cards from subtitle timecodes like subs2srs
- **Making dubtitlesÂ¹**: Make a subtitle file of dubs using Speech-To-Text
- **Voice enhancing**: Make voices louder than Music&Effects 
- **Subtitle romanizationÂ²**
- **Subtitle tokenization**: Separate words with spaces for languages which don't use spaces
- **Selective transliteration**: selective transliteration of subtitles based on [logogram](https://en.wikipedia.org/wiki/Logogram) frequency. Currently only japanese Kanjis are supported. Kanji with a frequency rank below the user-defined frequency threshold and regular readings are preserved, while others are converted to hiragana.

<sub> Â¹ 'dubtitles' is a *subtitle file that matches the dubbing lines exactly*. It is needed because translations of dubbings and of subtitles differ, as explained [here](https://www.quora.com/Why-do-subtitles-on-a-lot-of-dubbed-shows-not-match-up-with-the-dub-itself)</sup>

<sup> Â² for the list of supported languages by the transliteration feature see [here](https://github.com/tassa-yoniso-manasi-karoto/translitkit?tab=readme-ov-file#currently-implemented-tokenizers--transliterators) </sub>
<br>
<br>

> [!NOTE]
> **Some features require an API key because certain processing tasks, such as speech-to-text, audio enhancement, are outsourced to an external provider** like Replicate. These companies offer cloud-based machine learning models that handle complex tasks remotely, allowing Langkit to leverage the models without requiring local computation. <br> The cost of running a few processing tasks using these models is typically very low. 

> [!WARNING]
> âš ï¸ **about Feature Combinations**: langkit provides numerous features, some of which may overlap or influence each other's behavior, creating a complex network of conditional interactions. Although relatively extensive testing has been conducted, the multitude of possible combinations mean that certain specific scenarios *will* still contain bugs / unexpected behavior. Users are encouraged to **report any issues encountered either with the debug info exported from the Setting panel or with the crash report log**, especially when utilizing less common or more intricate feature combinations.

# tldr cli

```
ğ—•ğ—®ğ˜€ğ—¶ğ—° ğ˜€ğ˜‚ğ—¯ğ˜€ğŸ®ğ˜€ğ—¿ğ˜€ ğ—³ğ˜‚ğ—»ğ—°ğ˜ğ—¶ğ—¼ğ—»ğ—®ğ—¹ğ—¶ğ˜ğ˜†
$ langkit subs2cards media.mp4 media.th.srt media.en.srt

ğ—•ğ˜‚ğ—¹ğ—¸ ğ—½ğ—¿ğ—¼ğ—°ğ—²ğ˜€ğ˜€ğ—¶ğ—»ğ—´ ğ˜„ğ—¶ğ˜ğ—µ ğ—®ğ˜‚ğ˜ğ—¼ğ—ºğ—®ğ˜ğ—¶ğ—° ğ˜€ğ˜‚ğ—¯ğ˜ğ—¶ğ˜ğ—¹ğ—² ğ˜€ğ—²ğ—¹ğ—²ğ—°ğ˜ğ—¶ğ—¼ğ—» (ğ˜©ğ˜¦ğ˜³ğ˜¦: ğ˜­ğ˜¦ğ˜¢ğ˜³ğ˜¯ ğ˜£ğ˜³ğ˜¢ğ˜»ğ˜ªğ˜­ğ˜ªğ˜¢ğ˜¯ ğ˜±ğ˜°ğ˜³ğ˜µğ˜¶ğ˜¨ğ˜¦ğ˜´ğ˜¦ ğ˜§ğ˜³ğ˜°ğ˜® ğ˜¤ğ˜¢ğ˜¯ğ˜µğ˜°ğ˜¯ğ˜¦ğ˜´ğ˜¦ ğ˜°ğ˜³ ğ˜ªğ˜§ ğ˜¶ğ˜¯ğ˜¢ğ˜·ğ˜¢ğ˜ªğ˜­ğ˜¢ğ˜£ğ˜­ğ˜¦, ğ˜µğ˜³ğ˜¢ğ˜¥ğ˜ªğ˜µğ˜ªğ˜°ğ˜¯ğ˜¢ğ˜­ ğ˜¤ğ˜©ğ˜ªğ˜¯ğ˜¦ğ˜´ğ˜¦)
$ langkit subs2cards media.mp4 -l "pt-BR,yue,zh-Hant"

ğ—¦ğ˜‚ğ—¯ğ˜ğ—¶ğ˜ğ—¹ğ—² ğ˜ğ—¿ğ—®ğ—»ğ˜€ğ—¹ğ—¶ğ˜ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—» (+ğ˜ğ—¼ğ—¸ğ—²ğ—»ğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—¶ğ—³ ğ—»ğ—²ğ—°ğ—²ğ˜€ğ˜€ğ—®ğ—¿ğ˜†)
$ langkit translit media.ja.srt

ğ— ğ—®ğ—¸ğ—² ğ—®ğ—» ğ—®ğ˜‚ğ—±ğ—¶ğ—¼ğ˜ğ—¿ğ—®ğ—°ğ—¸ ğ˜„ğ—¶ğ˜ğ—µ ğ—²ğ—»ğ—µğ—®ğ—»ğ—°ğ—²ğ—±/ğ—®ğ—ºğ—½ğ—¹ğ—¶ğ—³ğ—¶ğ—²ğ—± ğ˜ƒğ—¼ğ—¶ğ—°ğ—²ğ˜€ ğ—³ğ—¿ğ—¼ğ—º ğ˜ğ—µğ—² ğŸ®ğ—»ğ—± ğ—®ğ˜‚ğ—±ğ—¶ğ—¼ğ˜ğ—¿ğ—®ğ—°ğ—¸ ğ—¼ğ—³ ğ˜ğ—µğ—² ğ—ºğ—²ğ—±ğ—¶ğ—® (ğ˜™ğ˜¦ğ˜±ğ˜­ğ˜ªğ˜¤ğ˜¢ğ˜µğ˜¦ ğ˜ˆğ˜—ğ˜ ğ˜µğ˜°ğ˜¬ğ˜¦ğ˜¯ ğ˜¯ğ˜¦ğ˜¦ğ˜¥ğ˜¦ğ˜¥)
$ langkit enhance media.mp4 -a 2 --sep demucs

ğ— ğ—®ğ—¸ğ—² ğ—±ğ˜‚ğ—¯ğ˜ğ—¶ğ˜ğ—¹ğ—²ğ˜€ ğ˜‚ğ˜€ğ—¶ğ—»ğ—´ ğ—¦ğ—½ğ—²ğ—²ğ—°ğ—µ-ğ˜ğ—¼-ğ—§ğ—²ğ˜…ğ˜ ğ—¼ğ—» ğ˜ğ—µğ—² ğ˜ğ—¶ğ—ºğ—²ğ—°ğ—¼ğ—±ğ—²ğ˜€ ğ—¼ğ—³ ğ—½ğ—¿ğ—¼ğ˜ƒğ—¶ğ—±ğ—²ğ—± ğ˜€ğ˜‚ğ—¯ğ˜ğ—¶ğ˜ğ—¹ğ—² ğ—³ğ—¶ğ—¹ğ—² (ğ˜™ğ˜¦ğ˜±ğ˜­ğ˜ªğ˜¤ğ˜¢ğ˜µğ˜¦ ğ˜ˆğ˜—ğ˜ ğ˜µğ˜°ğ˜¬ğ˜¦ğ˜¯ ğ˜¯ğ˜¦ğ˜¦ğ˜¥ğ˜¦ğ˜¥)
$ langkit subs2dubs --stt whisper media.mp4 (media.th.srt) -l "th"

ğ—–ğ—¼ğ—ºğ—¯ğ—¶ğ—»ğ—² ğ—®ğ—¹ğ—¹ ğ—¼ğ—³ ğ˜ğ—µğ—² ğ—®ğ—¯ğ—¼ğ˜ƒğ—² ğ—¶ğ—» ğ—¼ğ—»ğ—² ğ—°ğ—¼ğ—ºğ—ºğ—®ğ—»ğ—±
$ langkit subs2cards /path/to/media/dir/  -l "th,en" --stt whisper --sep demucs --translit
```

# Features in detail

## Subs2cards
Subs2cards converts your favorite TV shows and movies directly into Anki flashcards by extracting dialogues, images, and audio clips based on subtitle timecodes. It's ideal for  sentence mining and context-aware word memorization. 

<details>
<summary> 
    
#### Details
</summary>

#### Extra features compared to subs2srs

- **Default encoding to OPUS / AVIF**: Use modern codecs to save storage.
- **Parallelization / multi-threading by default**: By default all CPU cores available are used. You can reduce CPU usage by passing a lower ```--workers``` value than the default.
- **Bulk / recursive directory processing**: if you pass a directory instead of a mp4. The target and native language must be set using ```-l```, see tldr section.
- **Seamless resumption of previously interrupted runs**

</details>

## Dubtitles
Creates accurate subtitle files specifically synchronized with dubbed audio tracks using speech-to-text. This addresses the common mismatch between subtitle translations and audio dubs, ensuring text follows closely spoken dialogue.

<details>
<summary> 
    
#### Details
</summary>

By default **a dubtitle file will also be created from these transcriptions.**

AFAIK Language Reactor was the first to combine this with language learning from content however I found the accuracy of the STT they use to be unimpressive.
    
| Name (to be passed with --stt) | Word Error Rate average across all supported langs (june 2024) | Number of languages supported        | Price        | Type        | Note                                                                                                                                                                                                                                                                                                                   |
|--------------------------------|-----------------|-------------------------------------------------------------------------------------|--------------|-------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| whisper, wh                    | 10,3%           | [57](https://platform.openai.com/docs/guides/speech-to-text/supported-languages%5C) | $1.1/1000min | MIT         | **See [here](https://github.com/openai/whisper/discussions/1762) for a breakdown of WER per language.**  |
| insanely-fast-whisper, fast    | 16,2%           | 57                                                                                  | $0.0071/run  | MIT         |                                                                                                                                                                                                                                                                                                                        |
| universal-1, u1                | 8,7%            | [17](https://www.assemblyai.com/docs/getting-started/supported-languages)           | $6.2/1000min | proprietary | **Untested** (doesn't support my target lang)                                                                                                                                                                                                                                                                          |

See  [ArtificialAnalysis](https://artificialanalysis.ai/speech-to-text) and [Amgadoz @Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1brqwun/i_compared_the_different_open_source_whisper/) for detailed comparisons.

Note: OpenAI just released a [turbo](https://github.com/openai/whisper/discussions/1762) model of large-v3 but they say it's on a par with large-v2 as far as accuracy is concerned so I won't bother to add it.


</details>

## Voice Enhancing

Boosts clarity of speech in audio tracks by amplifying voices while reducing background music and effects. Ideal for learners who struggle with distinguishing words clearly, particularly useful for tonal languages or when studying languages with dense or unfamiliar phonetic patterns.

<details>
<summary> 
    
#### Details
</summary>
This feature works by merging the original audiotrack with negative gain together with an audiotrack containing the voices only with additional gain.

The isolated voice track are obtained using one of these:

| Name (to be passed with --sep) | Quality of separated vocals | Price                               | Type        | Note                                                                                                                                 |
|--------------------------------|-----------------------------|-------------------------------------|-------------|--------------------------------------------------------------------------------------------------------------------------------------|
| demucs, de                     | good                        | very cheap 0.063$/run               | MIT license | **Recommended**                                                                                                            |
| demucs_ft, ft                  | good                        | cheap 0.252$/run                    | MIT license | Fine-tuned version: "take 4 times more time but might be a bit better". I couldn't hear any difference with the original in my test. | 
| spleeter, sp                   | rather poor                 | very, very cheap 0.00027$/run       | MIT license |                                                                                                                                      |
| elevenlabs, 11, el             | good                        | very, very expensive<br>1$/*MINUTE* | proprietary | Not supported on the GUI. Not fully supported on the CLI due to limitations of their API (mp3 only) which desync the processed audio with the original.<br> **Requires an Elevenlabs API token.** <br> Does more processing than the others: noises are entirely eliminated, but it distort the soundstage to put the voice in the center. It might feel a bit uncanny in an enhanced track. |

</details>

## Subtitle romanization
Convert subtitles into a roman character version as phonetically accurate as possible

<details>
<summary> 
    
#### Details
</summary>
The list of supported languages by the transliteration feature is [here](https://github.com/tassa-yoniso-manasi-karoto/translitkit?tab=readme-ov-file#currently-implemented-tokenizers--transliterators)
</details>

## Subtitle tokenization

Separate words with spaces for languages which don't use spaces

<details>
<summary> 
    
#### Details
</summary>
The list of supported languages by the tokenization feature is [here](https://github.com/tassa-yoniso-manasi-karoto/translitkit?tab=readme-ov-file#currently-implemented-tokenizers--transliterators)
</details>


## Selective (Kanji) Transliteration

Automatically transliterates Japanese subtitles from kanji into hiragana based on user-defined frequency thresholds and phonetic regularity. This feature helps Japanese learners focus on common kanji by selectively converting rarer or irregular characters into easier-to-read hiragana, facilitating incremental kanji learning and smoother immersion into native content.

<details>
<summary> 
    
#### Details
</summary>
The frequency list comes from 6th edition of "Remembering the Kanji" by James W. Heisig and supports the most 3000 frequent Kanjis.
</details>


<!-- ## (âš ï¸fixmeâš ï¸) Condensed Audio
langkit will automatically **make an audio file containing all the audio snippets of dialog** in the audiotrack. <br>
This is meant to be used for passive listening. <br>
More explanations and context here: [Optimizing Passive Immersion: Condensed Audio - YouTube](https://www.youtube.com/watch?v=QOLTeO-uCYU) -->


# Requirements
This fork require FFmpeg **v6 or higher (dev builds being preferred)**, Mediainfo, a [Replicate](https://replicate.com/home) API token.

The FFmpeg dev team recommends end-users to use only the latest [builds from the dev branch (master builds)](https://github.com/BtbN/FFmpeg-Builds/releases). The FFmpeg binary's location can be provided by a flag, in $PATH or in a "bin" directory placed in the folder where langkit is.

The static FFmpeg builds guarantee that you have up-to-date codecs. **If you don't use a well-maintained bleeding edge distro or brew, use the dev builds.** You can check your distro [here](https://repology.org/project/ffmpeg/versions).

## API Keys
At the moment tokens should be passed through these env variables: REPLICATE_API_TOKEN, ASSEMBLYAI_API_KEY, ELEVENLABS_API_TOKEN.

$$
\color{red}
\text{TODO}
$$


# FAQ

<details>
<summary> 

#### Why isn't there the possibility to run the speech-to-text or voice separation locally?
</summary>

Because I only have a 10 year old Pentium CPU with a graphic chipset.
</details>
<details>
<summary> 
    
#### Why use subs2srs approach nowdays?
</summary>

There are plenty of alternative comprehensible input companion already: [Language Reactor](https://www.languagereactor.com/) (previously Language Learning With Netflix), [asbplayer](https://github.com/killergerbah/asbplayer), [mpvacious](https://github.com/Ajatt-Tools/mpvacious), [voracious](https://github.com/rsimmons/voracious), [memento](https://github.com/ripose-jp/Memento)...

They are awesome but all of them are media-centric: they are implemented around watching shows.

The approach I take for card-making here is language-centric:
- **word-centric notes referencing all common meanings**: I cross-source dictionaries, LLMs to map the meanings, connotations, register of a word. Searching my database of generated TSV I can illustrate & disambiguate with real-world examples the meanings I have found. This results in high quality notes regrouping all examples sentences, TTS, picture... and any other fields related to the word, allowing for maximum context.
- **word-note reuse for language laddering**: another advantage of this approach it that you can use this very note as basis for making cards for a new target language further down the line, while keeping all your previous note fields at hand for making the cards template for your new target language. The initial language acts just like Note ID for a meaning mapped across multiple languages. The majority of the basic vocabulary can be translated across languages directly with no real loss of meaning (and you can go on to disambiguate it further, using the method above for example). The effort that you spend on your first target language will thus pay off on subsequent languages.

There are several additional tools I made to accomplish this but they are hardcoded messes and not meant to be published.

**Future developement could entirely automate the process of making cards described above using LLMs.**
</details>

## Output

Before you can import the deck with Anki though, you must add a new
[Note Type](https://docs.ankiweb.net/#/editing?id=adding-a-note-type)
which includes some or all of the fields below on the front and/or back of
each card. The columns in the generated `.tsv` file are as follows:

| # | Name | Description |
| - | ---- | ----------- |
| 1 | Sound | Extracted audio as a `[sound]` tag for Anki |
| 2 | Time | Subtitle start time code as a string |
| 3 | Source | Base name of the subtitle source file |
| 4 | Image | Selected image frame as an `<img>` tag |
| 5 | ForeignCurr | Current text in foreign subtitles file |
| 6 | NativeCurr | Current text in native subtitles file |
| 7 | ForeignPrev | Previous text in foreign subtitles file |
| 8 | NativePrev | Previous text in native subtitles file |
| 9 | ForeignNext | Next text in foreign subtitles file |
| 10 | NativeNext | Next text in native subtitles file |

When you review the created deck for the first time, you should go quickly
through the entire deck at once. During this first pass, your goal should be
to identify those cards which you can understand almost perfectly, if not for
the odd piece of unknown vocabulary or grammar; all other cards which are
either too hard or too easy should be deleted in this pass. Any cards which
remain in the imported deck after mining should be refined and moved into your
regular deck for studying the language on a daily basis.

# Build & Development

See [DEV.md](https://github.com/tassa-yoniso-manasi-karoto/langkit/blob/master/DEV.md)

# Aknowledgements
Fork of Bunkai, which reimplemented the functionality first pioneered by **cb4960** with [subs2srs](https://subs2srs.sourceforge.net/).

$$
\color{red}
\text{TODO}
$$




# License
All new contributions from commit d540bd4 onward are licensed under **GPL-3.0**.
